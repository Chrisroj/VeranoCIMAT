{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ab969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731aa760",
   "metadata": {},
   "source": [
    "**Pasos a seguir**\n",
    "1. Carga y manipulaci√≥n de tweets\n",
    "2. Limpiar cada tweet (Eliminar car√°cteres no alfanumericos)\n",
    "3. Tokenizaci√≥n\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5e443",
   "metadata": {},
   "source": [
    "# 1 Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc88b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos previamente descargados en Noviembre 2017\n",
    "url = 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-con-R/master/datos/'\n",
    "tweets_elon   = pd.read_csv(url + \"datos_tweets_@elonmusk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d07d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-09T17:28:57Z</td>\n",
       "      <td>9.286758e+17</td>\n",
       "      <td>\"If one day, my words are against science, cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-09T17:12:46Z</td>\n",
       "      <td>9.286717e+17</td>\n",
       "      <td>I placed the flowers\\n\\nThree broken ribs\\nA p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-08T18:55:13Z</td>\n",
       "      <td>9.283351e+17</td>\n",
       "      <td>Atat√ºrk Anƒ±tkabir https://t.co/al3wt0njr6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-07T19:48:45Z</td>\n",
       "      <td>9.279862e+17</td>\n",
       "      <td>@Bob_Richards One rocket, slightly toasted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-28T21:36:18Z</td>\n",
       "      <td>9.243894e+17</td>\n",
       "      <td>@uncover007 500 ft so far. Should be 2 miles l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>2013-03-20T00:53:40Z</td>\n",
       "      <td>3.141779e+17</td>\n",
       "      <td>Testing separation of F9 rocket fairing (can h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>2013-03-19T03:03:05Z</td>\n",
       "      <td>3.138481e+17</td>\n",
       "      <td>Sharing a metaphysical milkshake with @RainnWi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>2013-03-17T18:32:54Z</td>\n",
       "      <td>3.133573e+17</td>\n",
       "      <td>@JBSiegelMD Cool, I'm glad you like it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676</th>\n",
       "      <td>2013-03-17T18:20:24Z</td>\n",
       "      <td>3.133541e+17</td>\n",
       "      <td>Craig Venter talks about flu vaccines and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>2013-03-10T00:39:02Z</td>\n",
       "      <td>3.105503e+17</td>\n",
       "      <td>Using √úber to order a Tesla Model S @SXSW with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2678 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     fecha            id  \\\n",
       "0     2017-11-09T17:28:57Z  9.286758e+17   \n",
       "1     2017-11-09T17:12:46Z  9.286717e+17   \n",
       "2     2017-11-08T18:55:13Z  9.283351e+17   \n",
       "3     2017-11-07T19:48:45Z  9.279862e+17   \n",
       "4     2017-10-28T21:36:18Z  9.243894e+17   \n",
       "...                    ...           ...   \n",
       "2673  2013-03-20T00:53:40Z  3.141779e+17   \n",
       "2674  2013-03-19T03:03:05Z  3.138481e+17   \n",
       "2675  2013-03-17T18:32:54Z  3.133573e+17   \n",
       "2676  2013-03-17T18:20:24Z  3.133541e+17   \n",
       "2677  2013-03-10T00:39:02Z  3.105503e+17   \n",
       "\n",
       "                                                  texto  \n",
       "0     \"If one day, my words are against science, cho...  \n",
       "1     I placed the flowers\\n\\nThree broken ribs\\nA p...  \n",
       "2             Atat√ºrk Anƒ±tkabir https://t.co/al3wt0njr6  \n",
       "3            @Bob_Richards One rocket, slightly toasted  \n",
       "4     @uncover007 500 ft so far. Should be 2 miles l...  \n",
       "...                                                 ...  \n",
       "2673  Testing separation of F9 rocket fairing (can h...  \n",
       "2674  Sharing a metaphysical milkshake with @RainnWi...  \n",
       "2675            @JBSiegelMD Cool, I'm glad you like it!  \n",
       "2676  Craig Venter talks about flu vaccines and the ...  \n",
       "2677  Using √úber to order a Tesla Model S @SXSW with...  \n",
       "\n",
       "[2678 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets_elon[[\"created_at\", \"status_id\", \"text\"]]\n",
    "tweets.columns = ['fecha', 'id', 'texto']\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0538c",
   "metadata": {},
   "source": [
    "**Hacer unas gr√°ficas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326f184",
   "metadata": {},
   "source": [
    "# 2 Preprocesamiento y tokenizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12468496",
   "metadata": {},
   "source": [
    "## 2.1 Sin limpiar el tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e92bfa",
   "metadata": {},
   "source": [
    "NLTK provides a function called word_tokenize() for splitting strings into tokens (nominally words). It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate tokens. Contractions are split apart (e.g. ‚ÄúWhat's‚Äù becomes ‚ÄúWhat‚Äù ‚Äú's‚Äú)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d42e2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [``, If, one, day, ,, my, words, are, against,...\n",
       "1       [I, placed, the, flowers, Three, broken, ribs,...\n",
       "2       [Atat√ºrk, Anƒ±tkabir, https, :, //t.co/al3wt0njr6]\n",
       "3       [@, Bob_Richards, One, rocket, ,, slightly, to...\n",
       "4       [@, uncover007, 500, ft, so, far, ., Should, b...\n",
       "                              ...                        \n",
       "2673    [Testing, separation, of, F9, rocket, fairing,...\n",
       "2674    [Sharing, a, metaphysical, milkshake, with, @,...\n",
       "2675    [@, JBSiegelMD, Cool, ,, I, 'm, glad, you, lik...\n",
       "2676    [Craig, Venter, talks, about, flu, vaccines, a...\n",
       "2677    [Using, √úber, to, order, a, Tesla, Model, S, @...\n",
       "Name: texto, Length: 2678, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"texto\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24abb1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Esto',\n",
       " '$',\n",
       " 'es',\n",
       " '1',\n",
       " 'ejemplo',\n",
       " 'de',\n",
       " \"l'limpieza\",\n",
       " 'de6',\n",
       " 'TEXTO',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/rnHPgyhx4Z',\n",
       " '@',\n",
       " 'cienciadedatos',\n",
       " '#',\n",
       " 'textmining']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"Esto $ es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc7e41",
   "metadata": {},
   "source": [
    "Well, both tokenizers almost work the same way, to split a given sentence into words. But you can think of TweetTokenizer as a subset of word_tokenize. TweetTokenizer keeps hashtags intact while word_tokenize doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9522dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99d960a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "?TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7382b456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Esto',\n",
       " '$',\n",
       " 'es',\n",
       " '1',\n",
       " 'ejemplo',\n",
       " 'de',\n",
       " \"l'limpieza\",\n",
       " 'de6',\n",
       " 'TEXTO',\n",
       " 'https://t.co/rnHPgyhx4Z',\n",
       " '@cienciadedatos',\n",
       " '#textmining']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TweetTokenizer()\n",
    "t.tokenize(\"Esto $ es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7498353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [\", If, one, day, ,, my, words, are, against, ...\n",
       "1       [I, placed, the, flowers, Three, broken, ribs,...\n",
       "2           [Atat√ºrk, Anƒ±tkabir, https://t.co/al3wt0njr6]\n",
       "3       [@Bob_Richards, One, rocket, ,, slightly, toas...\n",
       "4       [@uncover007, 500, ft, so, far, ., Should, be,...\n",
       "                              ...                        \n",
       "2673    [Testing, separation, of, F9, rocket, fairing,...\n",
       "2674    [Sharing, a, metaphysical, milkshake, with, @R...\n",
       "2675    [@JBSiegelMD, Cool, ,, I'm, glad, you, like, i...\n",
       "2676    [Craig, Venter, talks, about, flu, vaccines, a...\n",
       "2677    [Using, √úber, to, order, a, Tesla, Model, S, @...\n",
       "Name: texto, Length: 2678, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"texto\"].apply(t.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef6017",
   "metadata": {},
   "source": [
    "# 3 An√°lisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6dd6a",
   "metadata": {},
   "source": [
    "# 4 An√°lisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f86013",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. It is fully open-sourced under the [MIT License] (we sincerely appreciate all attributions and readily accept most contributions, but please don‚Äôt hold us liable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dcd54b",
   "metadata": {},
   "source": [
    "If you use either the dataset or any of the VADER sentiment analysis tools (VADER sentiment lexicon or Python code for rule-based sentiment analysis engine) in your research, please cite the above paper. For example:\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "845645e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c4ee32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de08a358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0000\n",
       "1      -0.2263\n",
       "2       0.0000\n",
       "3       0.0000\n",
       "4       0.4019\n",
       "         ...  \n",
       "2673    0.0000\n",
       "2674    0.4215\n",
       "2675    0.7959\n",
       "2676   -0.4389\n",
       "2677    0.0000\n",
       "Name: texto, Length: 2678, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"texto\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "159aa76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.***************************** {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\n",
      "VADER is smart, handsome, and funny!***************************** {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'compound': 0.8439}\n",
      "VADER is very smart, handsome, and funny.************************ {'neg': 0.0, 'neu': 0.299, 'pos': 0.701, 'compound': 0.8545}\n",
      "VADER is VERY SMART, handsome, and FUNNY.************************ {'neg': 0.0, 'neu': 0.246, 'pos': 0.754, 'compound': 0.9227}\n",
      "VADER is VERY SMART, handsome, and FUNNY!!!********************** {'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.9342}\n",
      "VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!********* {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.9469}\n",
      "VADER is not smart, handsome, nor funny.************************* {'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.7424}\n",
      "The book was good.*********************************************** {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "At least it isn't a horrible book.******************************* {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.431}\n",
      "The book was only kind of good.********************************** {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\n",
      "The plot was good, but the characters are uncompelling and the dialog is not great. {'neg': 0.327, 'neu': 0.579, 'pos': 0.094, 'compound': -0.7042}\n",
      "Today SUX!******************************************************* {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\n",
      "Today only kinda sux! But I'll get by, lol*********************** {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\n",
      "Make sure you :) or :D today!************************************ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\n",
      "Catch utf-8 emoji such as such as üíò and üíã and üòÅ****************** {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.875}\n",
      "Not bad at all*************************************************** {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7fc9b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"{:-<20}\".format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9962bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07ad6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b93963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fa166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84ec38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291af480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746de28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee696c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ff8758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\n",
      "['esto', 'es', 'ejemplo', 'de', 'limpieza', 'de', 'texto', 'cienciadedatos', 'textmining']\n"
     ]
    }
   ],
   "source": [
    "def limpiar_tokenizar(texto):\n",
    "    '''\n",
    "    Esta funci√≥n limpia y tokeniza el texto en palabras individuales.\n",
    "    El orden en el que se va limpiando el texto no es arbitrario.\n",
    "    El listado de signos de puntuaci√≥n se ha obtenido de: print(string.punctuation)\n",
    "    y re.escape(string.punctuation)\n",
    "    '''\n",
    "    \n",
    "    # Se convierte todo el texto a min√∫sculas\n",
    "    nuevo_texto = texto.lower()\n",
    "    # Eliminaci√≥n de p√°ginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
    "    # Eliminaci√≥n de signos de puntuaci√≥n\n",
    "    regex = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
    "    # Eliminaci√≥n de n√∫meros\n",
    "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
    "    # Eliminaci√≥n de espacios en blanco m√∫ltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "    # Tokenizaci√≥n por palabras individuales\n",
    "    nuevo_texto = nuevo_texto.split(sep = ' ')\n",
    "    # Eliminaci√≥n de tokens con una longitud < 2\n",
    "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
    "    \n",
    "    return(nuevo_texto)\n",
    "\n",
    "test = \"Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
    "print(test)\n",
    "print(limpiar_tokenizar(texto=test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45521b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1404c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7796f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52f347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
