{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ab969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731aa760",
   "metadata": {},
   "source": [
    "**Pasos a seguir**\n",
    "1. Carga y manipulación de tweets\n",
    "2. Limpiar cada tweet (Eliminar carácteres no alfanumericos)\n",
    "3. Tokenización\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5e443",
   "metadata": {},
   "source": [
    "# 1 Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc88b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos previamente descargados en Noviembre 2017\n",
    "url = 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-con-R/master/datos/'\n",
    "tweets_elon   = pd.read_csv(url + \"datos_tweets_@elonmusk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d07d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-09T17:28:57Z</td>\n",
       "      <td>9.286758e+17</td>\n",
       "      <td>\"If one day, my words are against science, cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-09T17:12:46Z</td>\n",
       "      <td>9.286717e+17</td>\n",
       "      <td>I placed the flowers\\n\\nThree broken ribs\\nA p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-08T18:55:13Z</td>\n",
       "      <td>9.283351e+17</td>\n",
       "      <td>Atatürk Anıtkabir https://t.co/al3wt0njr6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-07T19:48:45Z</td>\n",
       "      <td>9.279862e+17</td>\n",
       "      <td>@Bob_Richards One rocket, slightly toasted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-28T21:36:18Z</td>\n",
       "      <td>9.243894e+17</td>\n",
       "      <td>@uncover007 500 ft so far. Should be 2 miles l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>2013-03-20T00:53:40Z</td>\n",
       "      <td>3.141779e+17</td>\n",
       "      <td>Testing separation of F9 rocket fairing (can h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>2013-03-19T03:03:05Z</td>\n",
       "      <td>3.138481e+17</td>\n",
       "      <td>Sharing a metaphysical milkshake with @RainnWi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>2013-03-17T18:32:54Z</td>\n",
       "      <td>3.133573e+17</td>\n",
       "      <td>@JBSiegelMD Cool, I'm glad you like it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676</th>\n",
       "      <td>2013-03-17T18:20:24Z</td>\n",
       "      <td>3.133541e+17</td>\n",
       "      <td>Craig Venter talks about flu vaccines and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>2013-03-10T00:39:02Z</td>\n",
       "      <td>3.105503e+17</td>\n",
       "      <td>Using Über to order a Tesla Model S @SXSW with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2678 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     fecha            id  \\\n",
       "0     2017-11-09T17:28:57Z  9.286758e+17   \n",
       "1     2017-11-09T17:12:46Z  9.286717e+17   \n",
       "2     2017-11-08T18:55:13Z  9.283351e+17   \n",
       "3     2017-11-07T19:48:45Z  9.279862e+17   \n",
       "4     2017-10-28T21:36:18Z  9.243894e+17   \n",
       "...                    ...           ...   \n",
       "2673  2013-03-20T00:53:40Z  3.141779e+17   \n",
       "2674  2013-03-19T03:03:05Z  3.138481e+17   \n",
       "2675  2013-03-17T18:32:54Z  3.133573e+17   \n",
       "2676  2013-03-17T18:20:24Z  3.133541e+17   \n",
       "2677  2013-03-10T00:39:02Z  3.105503e+17   \n",
       "\n",
       "                                                  texto  \n",
       "0     \"If one day, my words are against science, cho...  \n",
       "1     I placed the flowers\\n\\nThree broken ribs\\nA p...  \n",
       "2             Atatürk Anıtkabir https://t.co/al3wt0njr6  \n",
       "3            @Bob_Richards One rocket, slightly toasted  \n",
       "4     @uncover007 500 ft so far. Should be 2 miles l...  \n",
       "...                                                 ...  \n",
       "2673  Testing separation of F9 rocket fairing (can h...  \n",
       "2674  Sharing a metaphysical milkshake with @RainnWi...  \n",
       "2675            @JBSiegelMD Cool, I'm glad you like it!  \n",
       "2676  Craig Venter talks about flu vaccines and the ...  \n",
       "2677  Using Über to order a Tesla Model S @SXSW with...  \n",
       "\n",
       "[2678 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets_elon[[\"created_at\", \"status_id\", \"text\"]]\n",
    "tweets.columns = ['fecha', 'id', 'texto']\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0538c",
   "metadata": {},
   "source": [
    "**Hacer unas gráficas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326f184",
   "metadata": {},
   "source": [
    "# 2 Preprocesamiento y tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12468496",
   "metadata": {},
   "source": [
    "## 2.1 Sin limpiar el tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e92bfa",
   "metadata": {},
   "source": [
    "NLTK provides a function called word_tokenize() for splitting strings into tokens (nominally words). It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate tokens. Contractions are split apart (e.g. “What's” becomes “What” “'s“)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d42e2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [``, If, one, day, ,, my, words, are, against,...\n",
       "1       [I, placed, the, flowers, Three, broken, ribs,...\n",
       "2       [Atatürk, Anıtkabir, https, :, //t.co/al3wt0njr6]\n",
       "3       [@, Bob_Richards, One, rocket, ,, slightly, to...\n",
       "4       [@, uncover007, 500, ft, so, far, ., Should, b...\n",
       "                              ...                        \n",
       "2673    [Testing, separation, of, F9, rocket, fairing,...\n",
       "2674    [Sharing, a, metaphysical, milkshake, with, @,...\n",
       "2675    [@, JBSiegelMD, Cool, ,, I, 'm, glad, you, lik...\n",
       "2676    [Craig, Venter, talks, about, flu, vaccines, a...\n",
       "2677    [Using, Über, to, order, a, Tesla, Model, S, @...\n",
       "Name: texto, Length: 2678, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"texto\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24abb1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Esto',\n",
       " '$',\n",
       " 'es',\n",
       " '1',\n",
       " 'ejemplo',\n",
       " 'de',\n",
       " \"l'limpieza\",\n",
       " 'de6',\n",
       " 'TEXTO',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/rnHPgyhx4Z',\n",
       " '@',\n",
       " 'cienciadedatos',\n",
       " '#',\n",
       " 'textmining']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"Esto $ es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc7e41",
   "metadata": {},
   "source": [
    "Well, both tokenizers almost work the same way, to split a given sentence into words. But you can think of TweetTokenizer as a subset of word_tokenize. TweetTokenizer keeps hashtags intact while word_tokenize doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9522dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99d960a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "?TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7382b456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Esto',\n",
       " '$',\n",
       " 'es',\n",
       " '1',\n",
       " 'ejemplo',\n",
       " 'de',\n",
       " \"l'limpieza\",\n",
       " 'de6',\n",
       " 'TEXTO',\n",
       " 'https://t.co/rnHPgyhx4Z',\n",
       " '@cienciadedatos',\n",
       " '#textmining']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TweetTokenizer()\n",
    "t.tokenize(\"Esto $ es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7498353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [\", If, one, day, ,, my, words, are, against, ...\n",
       "1       [I, placed, the, flowers, Three, broken, ribs,...\n",
       "2           [Atatürk, Anıtkabir, https://t.co/al3wt0njr6]\n",
       "3       [@Bob_Richards, One, rocket, ,, slightly, toas...\n",
       "4       [@uncover007, 500, ft, so, far, ., Should, be,...\n",
       "                              ...                        \n",
       "2673    [Testing, separation, of, F9, rocket, fairing,...\n",
       "2674    [Sharing, a, metaphysical, milkshake, with, @R...\n",
       "2675    [@JBSiegelMD, Cool, ,, I'm, glad, you, like, i...\n",
       "2676    [Craig, Venter, talks, about, flu, vaccines, a...\n",
       "2677    [Using, Über, to, order, a, Tesla, Model, S, @...\n",
       "Name: texto, Length: 2678, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"texto\"].apply(t.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef6017",
   "metadata": {},
   "source": [
    "# 3 Análisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6dd6a",
   "metadata": {},
   "source": [
    "# 4 Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f86013",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. It is fully open-sourced under the [MIT License] (we sincerely appreciate all attributions and readily accept most contributions, but please don’t hold us liable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dcd54b",
   "metadata": {},
   "source": [
    "If you use either the dataset or any of the VADER sentiment analysis tools (VADER sentiment lexicon or Python code for rule-based sentiment analysis engine) in your research, please cite the above paper. For example:\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "845645e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c4ee32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de08a358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0000\n",
       "1      -0.2263\n",
       "2       0.0000\n",
       "3       0.0000\n",
       "4       0.4019\n",
       "         ...  \n",
       "2673    0.0000\n",
       "2674    0.4215\n",
       "2675    0.7959\n",
       "2676   -0.4389\n",
       "2677    0.0000\n",
       "Name: texto, Length: 2678, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"texto\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "159aa76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.***************************** {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\n",
      "VADER is smart, handsome, and funny!***************************** {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'compound': 0.8439}\n",
      "VADER is very smart, handsome, and funny.************************ {'neg': 0.0, 'neu': 0.299, 'pos': 0.701, 'compound': 0.8545}\n",
      "VADER is VERY SMART, handsome, and FUNNY.************************ {'neg': 0.0, 'neu': 0.246, 'pos': 0.754, 'compound': 0.9227}\n",
      "VADER is VERY SMART, handsome, and FUNNY!!!********************** {'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.9342}\n",
      "VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!********* {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.9469}\n",
      "VADER is not smart, handsome, nor funny.************************* {'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.7424}\n",
      "The book was good.*********************************************** {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "At least it isn't a horrible book.******************************* {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.431}\n",
      "The book was only kind of good.********************************** {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\n",
      "The plot was good, but the characters are uncompelling and the dialog is not great. {'neg': 0.327, 'neu': 0.579, 'pos': 0.094, 'compound': -0.7042}\n",
      "Today SUX!******************************************************* {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\n",
      "Today only kinda sux! But I'll get by, lol*********************** {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\n",
      "Make sure you :) or :D today!************************************ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\n",
      "Catch utf-8 emoji such as such as 💘 and 💋 and 😁****************** {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.875}\n",
      "Not bad at all*************************************************** {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7fc9b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"{:-<20}\".format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9962bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07ad6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b93963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fa166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84ec38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291af480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746de28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee696c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ff8758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\n",
      "['esto', 'es', 'ejemplo', 'de', 'limpieza', 'de', 'texto', 'cienciadedatos', 'textmining']\n"
     ]
    }
   ],
   "source": [
    "def limpiar_tokenizar(texto):\n",
    "    '''\n",
    "    Esta función limpia y tokeniza el texto en palabras individuales.\n",
    "    El orden en el que se va limpiando el texto no es arbitrario.\n",
    "    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)\n",
    "    y re.escape(string.punctuation)\n",
    "    '''\n",
    "    \n",
    "    # Se convierte todo el texto a minúsculas\n",
    "    nuevo_texto = texto.lower()\n",
    "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
    "    # Eliminación de signos de puntuación\n",
    "    regex = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
    "    # Eliminación de números\n",
    "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
    "    # Eliminación de espacios en blanco múltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "    # Tokenización por palabras individuales\n",
    "    nuevo_texto = nuevo_texto.split(sep = ' ')\n",
    "    # Eliminación de tokens con una longitud < 2\n",
    "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
    "    \n",
    "    return(nuevo_texto)\n",
    "\n",
    "test = \"Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
    "print(test)\n",
    "print(limpiar_tokenizar(texto=test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45521b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1404c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7796f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52f347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
